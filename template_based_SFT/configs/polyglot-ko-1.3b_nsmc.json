{
    "output_dir" : "./checkpoints/polyglot-ko-1.3b-lora-nsmc",
    "model_name_or_path" : "EleutherAI/polyglot-ko-1.3b",
    "dataset_name" : "nsmc",
    "train_in_8bit" : false,
    "num_train_epochs" : 5,
    "max_len" : 96,
    "per_device_train_batch_size" : 256,
    "per_device_eval_batch_size" : 256,
    "fp16" : true,
    "overwrite_output_dir" : true,
    "gradient_accumulation_steps" : 1,
    "learning_rate" : 5e-05,
    "weight_decay" : 0.1,
    "logging_strategy" : "steps",
    "logging_steps" : 50,
    "save_strategy" : "no",
    "do_train" : true,
    "do_eval" : true,
    "report_to" : "wandb",
    "run_name" : "polyglot-ko-1.3b-nsmc",
    "seed" : 42,
    "data_seed" : 42
}