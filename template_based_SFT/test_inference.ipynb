{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from template_dataset import get_prompt_dataset, get_eval_dataloader, collate_fn\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/polyglot-ko-1.3b\",\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girinman/anaconda3/envs/hai-ground/lib/python3.9/site-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load adapters from the Hub and generate some output texts:\n",
    "\n",
    "peft_model_id = \"./checkpoints/polyglot-ko-1.3b-lora-nsmc/2023-03-21_03:46:37\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the Lora model\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id).to('cuda')\n",
    "# You can then directly use the trained model or the model that you have loaded from the 🤗 Hub for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: 다음 문장은 긍정일까요 부정일까요?\n",
      "아 이 영화 진짜 골때리네 정말 무슨 생각으로 개봉한거야?\n",
      "정답:부정\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"\"\"\n",
    "다음 문장은 긍정일까요 부정일까요?\n",
    "아 이 영화 진짜 골때리네 정말 무슨 생각으로 개봉한거야?\n",
    "정답:\n",
    "\"\"\".strip(), return_tensors=\"pt\").to('cuda')\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = peft_model.generate(\n",
    "        input_ids=batch['input_ids'].to(model.device),\n",
    "        max_new_tokens=2,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        pad_token_id = tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "print(f\"Generated: {tokenizer.decode(output_tokens[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset nsmc (/home/girinman/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf4b8f9dca64594b15f366992860bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/girinman/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-e490f3a13bf254f4.arrow\n",
      "Loading cached processed dataset at /home/girinman/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-d6184956744f867d.arrow\n"
     ]
    }
   ],
   "source": [
    "ids_to_labels = {0:\"부정\", 1:\"긍정\"}\n",
    "labels_to_ids = {v:k for k, v in ids_to_labels.items()}\n",
    "max_label_len = 2\n",
    "prefix = \"다음 문장은 긍정일까요 부정일까요?\\n\"\n",
    "suffix = \"\\n정답:\"\n",
    "columns = [\"document\", \"label\"]\n",
    "\n",
    "data = load_dataset(\"nsmc\")\n",
    "data = get_prompt_dataset(data, tokenizer, max_label_len=max_label_len, ids_to_labels=ids_to_labels)\n",
    "eval_dataloader = get_eval_dataloader(data['test'], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model, tokenizer, batch, first):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=batch['input_ids'].to(model.device),\n",
    "            attention_mask=batch['attention_mask'].to(model.device),\n",
    "            max_new_tokens=max_label_len,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id,\n",
    "        )\n",
    "    generated_txt = []\n",
    "    for i, g in enumerate(generated_ids):\n",
    "        decoded_txt = tokenizer.decode(g.tolist(), skip_special_tokens=True).split(suffix)\n",
    "        generated_txt.append(decoded_txt[-1].strip())\n",
    "    \n",
    "    labels = batch['decoded_labels']\n",
    "\n",
    "    if first:\n",
    "        count = 0\n",
    "        for gold, gen_txt in zip(labels, generated_txt):\n",
    "            print(f'gold: {ids_to_labels[gold]} pred: {gen_txt}')\n",
    "            count += 1\n",
    "            if count > 4:\n",
    "                break\n",
    "\n",
    "    return {'generated': generated_txt, 'labels': labels}\n",
    "\n",
    "def validation_epoch_end(outputs):\n",
    "    generated_txt = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for i in outputs:\n",
    "        generated_txt.extend(i['generated'])\n",
    "        labels.extend(i['labels'])\n",
    "        for txt in i['generated']:\n",
    "            try:\n",
    "                pred_id = labels_to_ids[txt]\n",
    "            except:\n",
    "                pred_id = -100\n",
    "            preds.append(pred_id)\n",
    "\n",
    "    class_ids = [key for key, value in ids_to_labels.items()]\n",
    "    is_binary = False\n",
    "    if -100 not in preds:\n",
    "        if (0 in class_ids) and (1 in class_ids) and (len(class_ids) == 2):\n",
    "            is_binary = True\n",
    "    else:\n",
    "        class_ids.append(-100)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    if is_binary:\n",
    "        f1 = f1_score(labels, preds)\n",
    "    else:\n",
    "        f1 = f1_score(y_true=labels, y_pred=preds, labels=class_ids, average=\"macro\")\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = acc\n",
    "    metrics['f1'] = f1\n",
    "    metrics['error'] = accuracy_score([-100] * len(preds), preds)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate(eval_dataloader):\n",
    "    eval_results = []\n",
    "    for i, batch in tqdm(enumerate(eval_dataloader), \"Generating predictions\", total=len(eval_dataloader)):\n",
    "        eval_results.append(validation_step(model, tokenizer, batch, i == 0))\n",
    "    return validation_epoch_end(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluation begins***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   1%|          | 1/196 [00:00<03:08,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold: 긍정 pred: 긍정\n",
      "gold: 부정 pred: 긍정\n",
      "gold: 부정 pred: 부정\n",
      "gold: 부정 pred: 부정\n",
      "gold: 부정 pred: 부정\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 196/196 [03:08<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Evaluation results***\n",
      "accuracy: 0.91062\n",
      "f1: 0.9110876788094622\n",
      "error rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"***Evaluation begins***\")\n",
    "metrics = evaluate(eval_dataloader)\n",
    "print(f\"***Evaluation results***\")\n",
    "print(f\"accuracy: {metrics['accuracy']}\")\n",
    "print(f\"f1: {metrics['f1']}\")\n",
    "print(f\"error rate: {metrics['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hai-ground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
